Version: 1.3.2
Author: Jonathan Gesell
Date: 19 October 2016

--SUMMARY --

The following is a summation of the listed version of the pipeline used by the Alkek Center for Metagenomics and Microbiome Research (CMMR).  The pipeline itself is split into two distinct modules: the first module creates useable fastq files from the Illumina BCL files, and the second is the analysis pipeline.  The functioning of these programs is detailed below.

--Process --

The following is a description of the above programs.

Fastq Creation:
	All BCL files are taken from the MiSeq and run through CASAVA, in order to generate individual fastq files for each sample.  Additionally, all reads that pass a read filter are compiled into monolithic read fastq files for both reads and barcodes.  All fastq files are then compressed using the pbzip2 program.
	PhiX is also removed from all reads, and the percent of reads that were mapped to PhiX is recorded as a percentage using Bowtie2.

Analysis Pipeline:
	Copies of the compressed fastq files are unzipped and undergo two separate merging filters using Usearch 7.0's fastq_mergepairs function.  Both of these merges ensure that there is a minimum overlap of 50 basepairs in each sample's reads, and with a truncation quality value of 5.  One of these merges, Standard, allows for a 4 basepair difference between the two reads, and is used in all future output requiring standard merge methods.  The second of these merges allows for no mismatches, and ensures that the overall merge does not exceed 254 basepairs in length; this merge, Strict, is used in most of our analyses.
	These merged files are then filtered further, using Usearch 7.0's fastq_filter program.  The first filter is applied to the Standard merge files listed above, allows for a maximum expected error of 0.5, and the outputs of this filter are labelled as FilteredRaw.  The second filter also uses the Standard fastqs, also allows for a maximum expected error of 0.5, relabels the fastq files for further processing; the outputs of this filter are listed as the FilteredStandard.  The final filter uses the strict fastq files, and only allows for a maximum expected error of 0.05, as well as relabeling the outputs; these are labeled as FilteredStrict.
	All three filtered files are then concatenated into one of three monolithic fastq files: strict, standard and raw.
	These three monolithic fastq files are then run through the bowtie2 program to ensure that all PhiX is stripped from the reads; resulting in three monolithic filtered fastq files.  The strict and standard monolithic fastq files are then converted to fasta files, while the raw remains as is.
	The fasta files created from the strict and standard merge files both undergo the same process in uparse:
		1) The fasta file is run through Usearch 7.0's derep_fulllength program, creating a uc file, and a dereplicated fasta file.
		2) The reads within the dereplicated fasta file are then sorted by size using Usearch 7.0's sortbysize program, and placed into a new fasta file.
		3) At increments of 0.4%, the sequences are run through Usearch 7.0's cluster_otus, creating a clustered fasta file at each increment.  These files are then filtered for any chimeras found by the program, and all chimera sequences are logged into their own file.  The decisions made by the clustering program are also logged in a file.  
		4) The output from the previous increment is then fed into the next iteration, until a maximum of 3.2% is reached.
		5) After the final run through the above loop, the final output is run through Usearch 7.0's uchime_ref program against the Gold database, using only the plus strand and allowing for no chimeras to create a file with no chimeras.
		6) This new file is run through Usearch 7.0's Usearch_global against the current Silva database, specifying ID to be set at 96.8%, using the plus strand, 0 maxaccepts and rejects.
		7) The previously created dereplicated fasta file is then searched for any singletons, which are stripped from this file and placed into a separate singleton fasta file.
		8) Using Usearch 7.0's Usearch_global program, the singleton fasta file is then compared to the previously created sorted fasta file, using only the plus strand, allowing for 32 maxaccepts and 128 maxrejects and requiring an identity value of 99%.  The results are stored.
		9) All the files created in the loop so far are then run through a program developed in-house that resolves the iterative uparse steps, creating an OTU table, removing the chimera and singleton reads, and using the Silva database to map them.
		10) The biom file is then summarized, and the statistics for the number of reads per sample that were mapped are recorded.  This file with the statistics is then merged with a file that was generated for the overall read statistics, to give a file that shows the number of raw reads, and mapped versus unmapped reads per sample.
	After the loop itself has finished, all reads are concatenated into a monolithic read files for read 1, read 2, and a combined reads fastq file.  The final step in the process is to recover the barcodes reads for any raw data analysis.  The monolithic fastqs are run through an in-house program that recovers these reads for delivery.
	At this point, all files required are moved into the appropriate deliverables folder.
	As a final step, R is called through Qiime to produce an Excel 2013 readable version of the biom files.

-- Results --

The final files that are created are a monolithic read 1 and read 2 file, a barcodes file for both the raw and standard merge, a monolithic read file of the standard merge, both a strict and standard merge OTU table and associated stats sheet, and an example Qiime mapping file.

-- List of Programs Used --
Bowtie2 (http://bowtie-bio.sourceforge.net/bowtie2/index.shtml)
GNU Parallel (https://www.gnu.org/software/parallel/)
pbzip2 (http://compression.ca/pbzip2/)
Perl v5.16.2 (https://metacpan.org/release/RJBS/perl-5.16.2)
Phyloseq v1.16.2 (https://joey711.github.io/phyloseq/install)
R v3.0.2 (https://cran.r-project.org/bin/windows/base/old/3.0.2/)
Silva v128 (https://www.arb-silva.de/documentation/release-128/)
Usearch v1.7 (http://www.drive5.com/Usearch/)
