Version: 1.1
Author: Jonathan Gesell
Date: 18 December 2015

--SUMMARY --

The following is a summation of the listed version of the pipeline used by the Alkek Center for Metagenomics and Microbiom Research (CMMR).  The pipeline itself is split into two distinct modules: MakeFastqsFromBCLs.pl and fullPipelineSplit.sh, which can be found linked below.

MakeFastqsFromBCLs:
https://github.com/jgesell/16S/blob/master/MakeFastqsFromBCLs.pl

fullPipelineSplit:
https://github.com/jgesell/16S/blob/master/fullPipelineSplit.sh

--Process --

The following is a description of the above programs.

MakeFastqsFromBCLs:
	All BCL files are taken from the MiSeq and run thru CASAVA, in order to generate individual fastq files for each sample.  Additionally, all reads that pass a read filter are compiled into monolithic read fastq files for both reads and barcodes.  All fastq files are then compressed using the pbzip2 program.
	PhiX is also removed from all reads, and the percent of reads that were mapped to PhiX is recorded as a percentage.

fullPipelineSplit:
	Copies of the compressed fastq files are unzipped and undergo two separate merging filters using usearch 70's fastq_mergepairs function.  Both of these merges ensure that there is a minimum overlap of 50 basepairs in each sample's reads, and with a truncation quality value of 5.  One of these merges, Standard, allows for a 4 basepair difference between the two reads, and is used in all future output requiring standard merge methods.  The second of these merges allows for no mismatches, and ensures that the overall merge does not exceed 254 basepairs in length; this merge, Strict, is used in most of our analysis.
	These merged files are then filtered further, using usearch 70's fastq_filter program.  The first filter is applied to the Standard merge files listed above, allows for a maximum expected error of 0.5, and the outputs of this filter are labelled as FilteredRaw.  The second filter also uses the Standard fastqs, also allows for a maximum expected error of 0.5, relables the fastq files for further processing; the outputs of this filter are listed as the FilteredStandard.  The final filter uses the Strict fastq files, and only allows for a maximum expected error of 0.05, as well as relabelling the outputs; these are lablelled as FilteredStrict.
	All three filtered files are then concatinated into one of three monolithic fastq files: strict, standard and raw.
	These three monolithic fastq files are then run thru the bowtie2 program to ensure that all PhiX is stripped from the reads; resulting in three monolithic filtered fastq files.  The strict and standard monolithic fastq files are then converted to fasta files, while the raw remains as is.
	The fasta files created from the strict and standard merge files both undergo the same process in uparse:
		1) The fasta file is run thru usearch 70's derep_fulllength program, creating a uc file, and a dereplicated fasta file.
		2) The reads within the dereplicated fasta file are then sorted by size using usearch70's sortbysize program, and placed into a new fasta file.
		3) At incriments of 0.4%, the sequences are run thru usearch70's cluster_otus, creating a clustered fasta file at each incriment.  These files are then filtered for any chimeras found by the program, adn all chimera sequences are logged into their own file.  The decisions made by the clustering program are also logged in a file.  
		4) The output from the previous incriment is then fed into the next, until a maximum of 3.2% is reached.
		5) After the final run thru the above loop, the final output is run thru usearch70's uchime_ref program against the Gold databse, using only the plus strand and allowing for no chimeras to create a file with no chimeras.
		6) This new file is run thru usearch70's usearch_global against the current Silva database, specifying ID to be set at 96.8%, using the plus strand, 0 maxaccepts and rejects.
		7) The previously created dereplicated fasta file is then searched for any signletons, which are stripped from this file and placed into a separate singleton fasta file.
		8) Using usearch70's usearch_global program, the singleton fasta file is then compared to the previously created sorted fasta file, using only the plus strand, allowing for 32 maxaccepts and 128 maxrejects and requiring an identity value of 99%.  The results are stored.
		9) All the files created in the loop so far are then run thru a program developed in-house that resolves the iterative uparse steps, creating an OTU table, removing the chimera and singleton reads, and using the Silva database to map them.
		10) The biom file is then summarized, and the statistics for the number of reads per sample that were mapped are recorded.  This file with the statistics is then merged with a file that was generated for the overal read statistics, to give a file that shows the number of mapped versus unmapped reads per sample.
	After the loop itself has finished, all reads are concatinated into a monolithic read files for read 1, read 2, and a combined reads fastq file.  The final step in the process is to recover the barcodes reads for any raw data analysis.  The monolithic fastqs are run thru an in-house program that recovers these reads  for delivery.
	At this point, all files required are moved into the appropriate deliverables folder.

-- Results --

The final files that are created are a monolithic read 1 and read 2 file, a barcodes file for both the raw and standard merge, a monolithic read file of the standard merge, both a strict and standard merge OTU table and associated stats sheet, and an example Qiime mapping file.